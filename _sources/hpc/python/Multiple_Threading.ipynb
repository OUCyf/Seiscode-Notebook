{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40606095",
   "metadata": {},
   "source": [
    "# Multiple Threading\n",
    "\n",
    "- Author: *{{Fu}}*\n",
    "- Update: *Dec 26, 2022*\n",
    "- Reading: *120 min*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Global Interpreter Lock\n",
    "\n",
    "Because of the existence of `Global Interpreter Lock (GIL)`, multi-threads code in multi-cores system can only run one thread at a time. Each thread needs to `acquire` a lock before it can run, and after the thread finishes running, the lock is `released`, and another thread acquires the lock again. So multiple threading is sutiable for `IO bound` mission, not the `CPU bound` mission\n",
    "\n",
    "```{figure} ../files/python_gil.webp\n",
    "---\n",
    "scale: 60%\n",
    "align: center\n",
    "name: python_gil\n",
    "---\n",
    "GIL in python\n",
    "```\n",
    "\n",
    "## threading\n",
    "\n",
    "Now let's introduce the `threading` package in python.\n",
    "\n",
    "\n",
    "### 1. create a thread, start a thread, and join function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4bb4bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_job start\n",
      "\n",
      "This is a thread of <Thread(thread_job, started 139862506202688)> \n",
      "\n",
      "thread_job finish\n",
      "\n",
      "There are 7 threads is running now\n",
      "\n",
      "The running threads list is: [<_MainThread(MainThread, started 139863201184640)>, <Thread(IOPub, started daemon 139863089256000)>, <Heartbeat(Heartbeat, started daemon 139862992746048)>, <Thread(Thread-3, started daemon 139862657201728)>, <Thread(Thread-4, started daemon 139862640420416)>, <ControlThread(Control, started daemon 139862623639104)>, <ParentPollerUnix(Thread-2, started daemon 139862522984000)>] \n",
      "\n",
      "The thread <_MainThread(MainThread, started 139863201184640)> is running now\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def thread_job():\n",
    "    print('thread_job start\\n')\n",
    "    print('This is a thread of %s \\n' % threading.current_thread())\n",
    "    print('thread_job finish\\n')\n",
    "\n",
    "def main():\n",
    "    # add a new thread\n",
    "    thread = threading.Thread(target=thread_job, name=\"thread_job\")\n",
    "\n",
    "    # start a thread\n",
    "    thread.start()\n",
    "\n",
    "    # wait this thread finished running, then run following code. Also called `block thread`\n",
    "    thread.join()\n",
    "\n",
    "    # see how many threads is running now\n",
    "    print(\"There are\", threading.active_count(), 'threads is running now\\n') \n",
    "\n",
    "    # see the thread list\n",
    "    print(\"The running threads list is:\", threading.enumerate(), '\\n') \n",
    "\n",
    "    # which one thread is running now\n",
    "    print(\"The thread\", threading.current_thread(), 'is running now\\n') \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c3f8c",
   "metadata": {},
   "source": [
    "### 2. quene function: get the results from multiple threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efeb6f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[1, 2, 3], [3, 4, 5], [4, 4, 4], [5, 5, 5]]\n",
      "Output: [[1, 4, 9], [9, 16, 25], [16, 16, 16], [25, 25, 25]]\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "# do a function: square a number\n",
    "def job(l,q):\n",
    "    new_list = l.copy()\n",
    "    for i in range(len(l)):\n",
    "        new_list[i] = l[i]**2\n",
    "    q.put(new_list)\n",
    "\n",
    "def multithreading():\n",
    "    # create a quene\n",
    "    q = Queue()\n",
    "\n",
    "    threads = []\n",
    "    data = [[1,2,3],[3,4,5],[4,4,4],[5,5,5]]\n",
    "\n",
    "    # create 4 threads\n",
    "    for i in range(4):\n",
    "        t = threading.Thread(target=job, args=(data[i], q))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    # get the results from multi-threads\n",
    "    results = []\n",
    "    for _ in range(4):\n",
    "          results.append(q.get())\n",
    "    print(\"Input:\", data)\n",
    "    print(\"Output:\", results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multithreading()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68119c",
   "metadata": {},
   "source": [
    "### 3. GIL is not suitable for `CPU bound` task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28256d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999998000000\n",
      "normal:  0.11132669448852539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999998000000\n",
      "multithreading:  0.09148931503295898\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import copy\n",
    "import time\n",
    "\n",
    "def job(l, q):\n",
    "    res = sum(l)\n",
    "    q.put(res)\n",
    "\n",
    "def multithreading(l):\n",
    "    q = Queue()\n",
    "    threads = []\n",
    "    for i in range(4):\n",
    "        t = threading.Thread(target=job, args=(copy.copy(l), q), name='T%i' % i)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    [t.join() for t in threads]\n",
    "    total = 0\n",
    "    for _ in range(4):\n",
    "        total += q.get()\n",
    "    print(total)\n",
    "\n",
    "def normal(l):\n",
    "    total = sum(l)\n",
    "    print(total)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    l = list(range(1000000))\n",
    "    s_t = time.time()\n",
    "    normal(l*4)\n",
    "    print('normal: ',time.time()-s_t)\n",
    "    s_t = time.time()\n",
    "    multithreading(l)\n",
    "    print('multithreading: ', time.time()-s_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718e2e3",
   "metadata": {},
   "source": [
    "### 4. GIL lock function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b835c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job1 1\n",
      "job1 2\n",
      "job1 3\n",
      "job1 4\n",
      "job1 5\n",
      "job1 6\n",
      "job1 7\n",
      "job1 8\n",
      "job1 9\n",
      "job1 10\n",
      "job2 20\n",
      "job2 30\n",
      "job2 40\n",
      "job2 50\n",
      "job2 60\n",
      "job2 70\n",
      "job2 80\n",
      "job2 90\n",
      "job2 100\n",
      "job2 110\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "def job1():\n",
    "    global A, lock\n",
    "    lock.acquire()\n",
    "    for i in range(10):\n",
    "        A += 1\n",
    "        print('job1', A)\n",
    "    lock.release()\n",
    "\n",
    "def job2():\n",
    "    global A, lock\n",
    "    lock.acquire()\n",
    "    for i in range(10):\n",
    "        A += 10\n",
    "        print('job2', A)\n",
    "    lock.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lock = threading.Lock()\n",
    "    A = 0\n",
    "    t1 = threading.Thread(target=job1)\n",
    "    t2 = threading.Thread(target=job2)\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t1.join()\n",
    "    t2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab1ea9",
   "metadata": {},
   "source": [
    "## ThreadPoolExecutor\n",
    "\n",
    "A `thread pool` maintains multiple threads waiting for tasks to be allocated for `concurrent` execution by the supervising program. By maintaining a pool of threads, the model increases performance and avoids latency in execution due to frequent creation and destruction of threads for short-lived tasks. The number of available threads is tuned to the computing resources available to the program, such as a parallel task queue after completion of execution.\n",
    "\n",
    "One benefit of a thread pool over creating a new thread for each task is that **thread creation and destruction overhead is restricted to the initial creation of the pool**, which may result in better performance and better system stability. Creating and destroying a thread and its associated resources can be an expensive process in terms of time. An excessive number of threads in reserve, however, wastes memory, and context-switching between the runnable threads invokes performance penalties.\n",
    "\n",
    "```{figure} ../files/thread_pool.png\n",
    "---\n",
    "scale: 20%\n",
    "align: center\n",
    "name: thread_pool\n",
    "---\n",
    "Thread pool in python\n",
    "```\n",
    "\n",
    "Now let's introduce `concurrent.futures` package in python.\n",
    "\n",
    "1. When `ThreadPoolExecutor` constructs an instance, pass in the `max_workers` parameter to set the maximum number of threads that can `run simultaneously` in the thread pool.\n",
    "2. Use the `submit` function to submit the task (function name and parameters) that the thread needs to execute to the thread pool, and return the handle of the task (similar to files and drawing). Note that submit is `not blocked`, and returns immediately.\n",
    "3. Through the task handle returned by the submit function, you can use the `done` method to determine whether the task is over. \n",
    "4. Use the `cancel` method to cancel the submitted task. If the task is already running in the thread pool, it cannot be canceled. In this example, the thread pool size is set to 0.2 and the task is already running, so the cancellation fails. If the size of the thread pool is changed to 0.1, then task1 is submitted first, and task2 is still waiting in line. At this time, it can be successfully canceled.\n",
    "5. Use the `result` method to get the return value of the task. Note: **result method is blocked**.\n",
    "\n",
    "\n",
    ":::{admonition} What's the different between `threading` and `concurrent.futures` packages?\n",
    "- `threading` is a common used package in python with a low-level API.\n",
    "- The `concurrent.futures` module encapsulates `thread` to provides a high-level interface for asynchronously executing callables. The asynchronous execution can be performed with threads, using `ThreadPoolExecutor`, or separate processes, using `ProcessPoolExecutor`. Both implement the same interface, which is defined by the abstract Executor class.\n",
    ":::\n",
    "\n",
    "\n",
    "### 1. create a thread pool, submit a task, get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270d6692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 1 is finished?： False\n",
      "cancel task 2： False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down video 0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down video 0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 1 is finished?： True\n",
      "task1's results:  0.2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def down_video(times):\n",
    "    time.sleep(times)\n",
    "    print(\"down video {}s finished\".format(times))\n",
    "    return times\n",
    "\n",
    "# create a thread pool with max_workers threads\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "# submit a task into thread pool，and submit function will return immediately with blocking\n",
    "task1 = executor.submit(down_video, (0.2))\n",
    "task2 = executor.submit(down_video, (0.1))\n",
    "\n",
    "# `done` function is used to check whether the task is finished\n",
    "print(\"task 1 is finished?：\",task1.done())\n",
    "\n",
    "# `cancel` function is used to cancel the task before the thread put into thread-pool \n",
    "print(\"cancel task 2：\",task2.cancel())\n",
    "\n",
    "time.sleep(1)\n",
    "print(\"task 1 is finished?：\",task1.done())\n",
    "\n",
    "# `result` is used to get the results of a thread\n",
    "print(\"task1's results: \", task1.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8148b32",
   "metadata": {},
   "source": [
    "### 2. as_completed function:\n",
    "\n",
    "Although the `done` function provides a method for judging whether the task is over, it is not very practical, because we don't know when the thread ends, and we need to always judge whether each task is over. At this time, you can use the `as_completed` method to retrieve the results of all tasks at once.\n",
    "\n",
    "The `as_completed` method is a generator that will block when no task is completed. When a certain task is completed, it can continue to execute the statement after the for loop, and then continue to block until all tasks end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "124f6b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download video 1 finished at 2023-01-30 23:27:00\n",
      "task 1 down load success\n",
      "download video 2 finished at 2023-01-30 23:27:00\n",
      "task 2 down load success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download video 3 finished at 2023-01-30 23:27:01\n",
      "download video 4 finished at 2023-01-30 23:27:01\n",
      "task 3 down load success\n",
      "task 4 down load success\n",
      "download video 5 finished at 2023-01-30 23:27:01\n",
      "task 5 down load success\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def download_video(index):\n",
    "    time.sleep(0.1)\n",
    "    print(\"download video {} finished at {}\".format(index,time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())))\n",
    "    return index\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "urls = [1, 2, 3, 4, 5]\n",
    "all_task = [executor.submit(download_video, (url)) for url in urls]\n",
    "\n",
    "for task in as_completed(all_task):\n",
    "    data = task.result()\n",
    "    print(\"task {} down load success\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a74ca7",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "5 tasks, 2 threads. Since a maximum of 2 threads are allowed to be executed at the same time when the thread pool is constructed, task 1 and task 2 are executed at the same time. Judging from the output of heavy code, after task 1 and task 2 are executed, the for loop Enter the blocking state, until the end of task 1 or task 2, for will continue to execute task 3 / task 4, and ensure that only two tasks are executed at the same time.\n",
    "\n",
    "\n",
    "### 3. map function:\n",
    "\n",
    "The difference from the `as_completed` method is that the `map` method can guarantee **the order of tasks**. For example: if you download 5 videos at the same time, even if the second video is downloaded before the first video, it will be blocked and wait for the first video to download. After the completion and notification of the main thread, the second downloaded video will be notified back to the main thread to ensure that the tasks are completed in order. Here is an example to illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "effd7aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download video 0.2 finished at 2023-01-30 23:27:01\n",
      "download video 0.3 finished at 2023-01-30 23:27:01\n",
      "task 0.3 down load success\n",
      "task 0.2 down load success\n",
      "download video 0.1 finished at 2023-01-30 23:27:01\n",
      "task 0.1 down load success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download video 0.4 finished at 2023-01-30 23:27:01\n",
      "task 0.4 down load success\n",
      "download video 0.5 finished at 2023-01-30 23:27:01\n",
      "task 0.5 down load success\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def download_video(index):\n",
    "    time.sleep(index)\n",
    "    print(\"download video {} finished at {}\".format(index,time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())))\n",
    "    return index\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "urls = [0.3, 0.2, 0.1, 0.4, 0.5]\n",
    "for data in executor.map(download_video,urls):\n",
    "    print(\"task {} down load success\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036ea52",
   "metadata": {},
   "source": [
    "### 4. wait function:\n",
    "\n",
    "The `wait` method is somewhat similar to the thread `join` method, which can `block` the main thread until all the threads in the thread pool have completed their operations.\n",
    "\n",
    "The `wait` method receives 3 parameters, the `waiting task sequence`, the `timeout time` and the `waiting condition`. The wait condition `return_when` defaults to `ALL_COMPLETED`, indicating that all tasks are to be completed. You can see that in the running results, all tasks are indeed completed, and the main thread prints out main. The waiting condition can also be set to `FIRST_COMPLETED`, indicating that the first task is completed and the wait is stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59af834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download video 0.1 finished at 2023-01-30 23:27:02\n",
      "download video 0.2 finished at 2023-01-30 23:27:02\n",
      "download video 0.3 finished at 2023-01-30 23:27:02\n",
      "download video 0.4 finished at 2023-01-30 23:27:02\n",
      "download video 0.5 finished at 2023-01-30 23:27:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main \n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED, FIRST_COMPLETED\n",
    "import time\n",
    "\n",
    "def download_video(index):\n",
    "    time.sleep(0.1)\n",
    "    print(\"download video {} finished at {}\".format(index,time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())))\n",
    "    return index\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "urls = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "all_task = [executor.submit(download_video,(url)) for url in urls]\n",
    "wait(all_task,return_when=ALL_COMPLETED)\n",
    "print(\"main \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81249375",
   "metadata": {},
   "source": [
    "## Http Downloder Project\n",
    "\n",
    "In this project, I use a multi-threaded method to download large file data, and support breakpoint transmission.\n",
    "\n",
    ":::{dropdown} Do you know how to transfer large data via `http`?\n",
    ":color: info\n",
    ":icon: info\n",
    "\n",
    "- The first thing we think of is **data compression**. Usually, when a browser sends a request, it will carry an `Accept-Encoding` header field, which contains a list of compression formats supported by the browser, such as `gzip`, `deflate`, `br`, etc., so that the server can choose one of them. The compression algorithm is put into the `Content-Encoding` response header, and the original data is compressed and sent to the browser.\n",
    "\n",
    "- `Chunked` transfer is to disassemble a large file, break it into multiple small blocks, and distribute these small blocks to the browser in batches, and the browser will assemble and restore them after receiving them. In this way, the browser and the server do not need to store all the files in the memory, only send and receive a small part each time, the network will not be occupied by large files for a long time, and resources such as memory and bandwidth are also saved. `Transfer-Encoding` and `Content-Length` are mutually exclusive, and these two fields cannot appear in the response message at the same time, and the transmission of a response message is either of known length or unknown length (chunked), this must be remembered.\n",
    "\n",
    "- `Range Requests` allow the client to use a dedicated field in the request header to indicate that only a part of the file is fetched. The header must include `Accept-Ranges: bytes`.\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{figure} ../files/http_transfer.png\n",
    "---\n",
    "scale: 60%\n",
    "align: center\n",
    "name: http_transfer\n",
    "---\n",
    "Transfer Big Data via Http \n",
    "```\n",
    "\n",
    "\n",
    ":::{admonition} Difference between `urllib` and `requests` package in python\n",
    "- `urllib3` is the most commonly used network service package in python.\n",
    "- `requests` uses `urllib3` under the hood and make it even simpler to make requests and retrieve data, and it aims for an easier-to-use API.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb3f894",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.url =  https://github.com/OUCyf/Latex-Template-Rice-USTC/raw/gh-pages/main.pdf \n",
      " self.resume =  True \n",
      " self.chunk_size =  102400 \n",
      " self.filename =  main.pdf \n",
      " self.num_threads =  4 \n",
      " self.proxies =  None \n",
      " self.timeout =  10 \n",
      " self.file_type =  application/octet-stream \n",
      " self.accept_ranges =  bytes \n",
      " self.content_length =  393563 \n",
      " self.transfer_encoding =  None \n",
      " self.header =  {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 6.0; Windows NT 6.1; Trident/4.1)'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from faker import Faker\n",
    "from retry import retry\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "cpu_cores = os.cpu_count()\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "\n",
    "class downloader_https():\n",
    "    def __init__(self, url, show_info = True, resume=True, filename=None, num_threads=cpu_cores, timeout=10, chunk_size=1024*1000, header=None, proxies=None):\n",
    "        \"\"\"\n",
    "        :param url: 下载地址\n",
    "        :param filename: 指定下载文件名 不指定则根据url截取命名\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.show_info = show_info\n",
    "        self.resume = resume\n",
    "        self.chunk_size = chunk_size  # 设置下载文件块大小 单位为字节（多线程下载时，一个线程下载一小块）\n",
    "        self.filename = filename\n",
    "        self.num_threads = num_threads\n",
    "        self.proxies = proxies\n",
    "        self.timeout = timeout\n",
    "        self.file_type = None\n",
    "        self.accept_ranges = None\n",
    "        self.content_length = None\n",
    "        self.transfer_encoding = None\n",
    "        if header is None:\n",
    "            self.header = {}\n",
    "            self.header.setdefault('User-Agent', Faker().user_agent())\n",
    "        elif 'User-Agent' not in header:\n",
    "            self.header.setdefault('User-Agent', Faker().user_agent())\n",
    "        else:\n",
    "            self.header = header\n",
    "\n",
    "    @retry(tries=3)\n",
    "    def check_url(self):\n",
    "        \"\"\"\n",
    "        判断url是否支持断点续传功能 and 是否支持多线程（文件内容是否为零）\n",
    "        \"\"\"\n",
    "        # 文件处理\n",
    "        _, filename = os.path.split(self.url)\n",
    "        self.filename = self.filename or filename\n",
    "\n",
    "        res = requests.head(self.url, headers=self.header, proxies=self.proxies, timeout=self.timeout, allow_redirects=True, verify=False)  # verify=False 关闭ssl双向验证，解决访问https报错问题\n",
    "\n",
    "        if not (200 <= res.status_code < 400):\n",
    "            raise Exception('Bad request!')\n",
    "\n",
    "        headers = res.headers\n",
    "        self.file_type = headers.get('Content-Type')\n",
    "        self.accept_ranges = headers.get('Accept-Ranges')\n",
    "        self.transfer_encoding = headers.get('Transfer-Encoding')\n",
    "\n",
    "        if self.transfer_encoding == \"chunked\" or self.transfer_encoding == \"gzip, chunked\":\n",
    "            self.num_threads = 1\n",
    "            self.content_length = 0\n",
    "        else:\n",
    "            lengths = headers.get('Content-Length')\n",
    "            if lengths == None:\n",
    "                self.content_length = 0\n",
    "            else:\n",
    "                self.content_length = int(lengths)\n",
    "\n",
    "\n",
    "    def get_range(self, start=0):\n",
    "        \"\"\"\n",
    "        根据设置的缓存大小以及文件大小划分字节序列\n",
    "        eg: [(0, 1023), (1024, 2047), (2048, 3071) ...]\n",
    "        \"\"\"\n",
    "        if self.transfer_encoding == \"chunked\" or self.transfer_encoding == \"gzip, chunked\":\n",
    "            _range = [(start, '')]\n",
    "        else:\n",
    "            lst = range(start, self.content_length, self.chunk_size)   \n",
    "            _range = list(zip(lst[:-1], [i - 1 for i in lst[1:]]))\n",
    "            _range.append((lst[-1], ''))\n",
    "\n",
    "        return _range\n",
    "\n",
    "\n",
    "    @retry(tries=5)\n",
    "    def download_by_piece(self, _range):\n",
    "        start, stop = _range\n",
    "        headers = {**self.header, **{\"Range\": f\"bytes={start}-{stop}\"}} # merge\n",
    "\n",
    "        res = requests.get(self.url, headers=headers, proxies=self.proxies, timeout=self.timeout, allow_redirects=True, verify=False)\n",
    "        if res.status_code != 206:\n",
    "            raise Exception(f'Request raise error, url: {self.url}, range: {_range}')\n",
    "        return _range, res.content\n",
    "\n",
    "\n",
    "    def download(self):\n",
    "        start = 0\n",
    "        self.check_url()\n",
    "\n",
    "        if self.accept_ranges != \"bytes\":\n",
    "            if self.show_info:\n",
    "                print(f'--- Mission ---: {self.url} download from scratch || with single thread, do not support breakpoint resuming')\n",
    "            \n",
    "            file_path = Path(self.filename)\n",
    "\n",
    "            res = requests.get(self.url, \n",
    "                                headers=self.header, \n",
    "                                proxies=self.proxies, \n",
    "                                timeout=self.timeout, \n",
    "                                allow_redirects=True, \n",
    "                                verify=False)\n",
    "\n",
    "            if res.status_code != 206:\n",
    "                raise Exception(f'Request raise error, url: {self.url}')\n",
    " \n",
    "            # 将下载的块写入文件\n",
    "            open(file_path, 'w').close()  # 生成0文件\n",
    "            with open(self.filename, 'rb+') as fp:\n",
    "                fp.seek(0)\n",
    "                fp.write(res.content)\n",
    "\n",
    "            if self.show_info:\n",
    "                print(f'--- File ---: {self.filename} download completely')\n",
    "        else:\n",
    "            file_path = Path(self.filename)\n",
    "\n",
    "            if self.resume:\n",
    "                open(file_path, 'w+').close()\n",
    "                start = 0\n",
    "                if self.show_info:\n",
    "                    print(f'--- Mission ---: {self.url} download from scratch || with {self.num_threads} threads, support breakpoint resuming')\n",
    "\n",
    "            else:\n",
    "                if file_path.exists():\n",
    "                    # 文件已存在 并且支持断点续传，可以从现有文件基础上继续下载\n",
    "                    start = file_path.lstat().st_size\n",
    "                    if self.show_info:\n",
    "                        print(f'--- Mission ---: {self.url} download from breakpoint || with {self.num_threads} threads, support breakpoint resuming')\n",
    "\n",
    "                    # If file have already downloaded \n",
    "                    if start == self.content_length:\n",
    "                        if self.show_info:\n",
    "                            print(f'--- File ---: {self.filename} has already been downloaded completely')\n",
    "                        return\n",
    "                else:\n",
    "                    open(file_path, 'w+').close() # 生成文件\n",
    "                    start = 0\n",
    "                    if self.show_info:\n",
    "                        print(f'--- Mission ---: {self.url} download from scratch || with {self.num_threads} threads, support breakpoint resuming')\n",
    "\n",
    "            # 初始化进度条\n",
    "            if self.show_info:\n",
    "                pbar = tqdm(total=self.content_length,\n",
    "                        initial=start,\n",
    "                        unit='B',\n",
    "                        unit_scale=True,\n",
    "                        desc=self.filename,\n",
    "                        unit_divisor=1024)\n",
    "            \n",
    "            # 使用多线池来创建线程\n",
    "            with ThreadPoolExecutor(max_workers=self.num_threads) as pool:\n",
    "                res = [pool.submit(self.download_by_piece, r) for r in self.get_range(start=start)] # or use map function\n",
    "\n",
    "                # 将下载的块写入文件\n",
    "                with open(self.filename, 'rb+') as fp:\n",
    "                    for item in as_completed(res):\n",
    "                        _range, content = item.result()\n",
    "                        start, stop = _range\n",
    "                        fp.seek(start)\n",
    "                        fp.write(content)\n",
    "                        # 更新进度条\n",
    "                        if self.show_info:\n",
    "                            pbar.update(self.chunk_size)\n",
    "\n",
    "            if self.show_info:\n",
    "                pbar.close()\n",
    "                print(f'--- File ---: {self.filename} download completely')\n",
    "\n",
    "\n",
    "    def print(self):\n",
    "        self.check_url()\n",
    "        print( \"self.url = \", self.url, '\\n',\n",
    "            \"self.resume = \", self.resume, '\\n',\n",
    "            \"self.chunk_size = \", self.chunk_size, '\\n',  # 设置下载文件块大小 单位为字节（多线程下载时，一个线程下载一小块）\n",
    "            \"self.filename = \", self.filename, '\\n',\n",
    "            \"self.num_threads = \", self.num_threads, '\\n',\n",
    "            \"self.proxies = \", self.proxies, '\\n',\n",
    "            \"self.timeout = \", self.timeout, '\\n',\n",
    "            \"self.file_type = \", self.file_type, '\\n',\n",
    "            \"self.accept_ranges = \", self.accept_ranges, '\\n',\n",
    "            \"self.content_length = \", self.content_length, '\\n',\n",
    "            \"self.transfer_encoding = \", self.transfer_encoding, '\\n',\n",
    "            \"self.header = \", self.header\n",
    "            )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = \"https://github.com/OUCyf/Latex-Template-Rice-USTC/raw/gh-pages/main.pdf\"\n",
    "    d = downloader_https(url, \n",
    "            num_threads = 4,\n",
    "            show_info = False,\n",
    "            resume = True, \n",
    "            chunk_size = 1024*100, \n",
    "            filename = None,  \n",
    "            header = None, \n",
    "            proxies = None, \n",
    "            timeout = 10)\n",
    "    d.download()\n",
    "    d.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205451f9",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. https://www.codersrc.com/archives/6707.html\n",
    "2. https://github.com/MorvanZhou/tutorials/tree/master/threadingTUT\n",
    "3. https://www.cnblogs.com/traditional/p/15373999.html\n",
    "4. https://www.51cto.com/article/665492.html\n",
    "5. https://github.com/wikizero/downloader"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   14,
   47,
   77,
   81,
   118,
   123,
   158,
   163,
   191,
   225,
   252,
   261,
   277,
   287,
   300,
   312,
   325,
   364,
   574
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}